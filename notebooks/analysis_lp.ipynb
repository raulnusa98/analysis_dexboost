{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *1. Load data and initial preprocessing* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Python\\my_projects\\dexboost\n",
      "Conexión exitosa a la base de datos.\n",
      "Conexión cerrada correctamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Cambiar directorio a carpeta del proyecto\n",
    "os.chdir('C:/Python/my_projects/dexboost/')\n",
    "print(os.getcwd())\n",
    "\n",
    "def conectar_db(ruta_db: str):\n",
    "    try:\n",
    "        conn = sqlite3.connect(ruta_db)\n",
    "        print(\"Conexión exitosa a la base de datos.\")\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error al conectar con la base de datos: {e}\")\n",
    "        raise\n",
    "\n",
    "def cerrar_conexion(conn):\n",
    "    try:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            print(\"Conexión cerrada correctamente.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error al cerrar la conexión: {e}\")\n",
    "        raise\n",
    "\n",
    "def cargar_tabla(conn, tabla):\n",
    "    try:\n",
    "        query = f\"SELECT * FROM {tabla}\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        return df\n",
    "    except sqlite3.DatabaseError as e:\n",
    "        print(f\"Error al cargar la tabla '{tabla}': {e}\")\n",
    "        raise\n",
    "\n",
    "# Conexión a la base de datos\n",
    "conn = conectar_db('data/main_2025-02-22_13-37-47.db')\n",
    "\n",
    "# Cargar tabla 'Analysis'\n",
    "data = cargar_tabla(conn, 'analysisLiquidityPool')\n",
    "\n",
    "# Cerrar conexión\n",
    "\n",
    "cerrar_conexion(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here we separate into 2 different dataframe, regarding IsLP column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'IsLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'IsLP'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     34\u001b[0m     df_boost \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsLP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_creation_lp, df_boost\n\u001b[1;32m---> 39\u001b[0m df_lp, df_boost \u001b[38;5;241m=\u001b[39m \u001b[43minitial_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m, in \u001b[0;36minitial_processing\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Format initial columns\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetectedAt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetectedAt\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m19\u001b[39m])\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsLP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIsLP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsPump\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsPump\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     26\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokenName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'IsLP'"
     ]
    }
   ],
   "source": [
    "def initial_processing(df):\n",
    "    \"\"\"\n",
    "    Preprocessing of analysisLiquidityPool table to be able to have quality data. \n",
    "    Separates Boost data and LP data regarding IsLP column. Resulting in 2 different DataFrames\n",
    "    \n",
    "    Args:\n",
    "        - df (pd.DataFrame): Initial data to be processed.\n",
    "    \n",
    "    Returns:\n",
    "        - Liquidity df (pd.DataFrame): data with LP creations\n",
    "        - Boosts df (pd.DataFrame): data with Boosts\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop ID column if exists\n",
    "    df.drop(columns='id', errors='ignore', inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Format initial columns\n",
    "    df['DetectedAt'] = pd.to_datetime(df['DetectedAt'].astype(str).str[:19])\n",
    "    df['IsLP'] = df['IsLP'].astype(bool)\n",
    "    df['IsPump'] = df['IsPump'].astype(bool)\n",
    "    df['TokenName'] = df['TokenName'].astype(str)\n",
    "    df['TokenMint'] = df['TokenMint'].astype(str)\n",
    "    df['TotalLiquidity'] = df['TotalLiquidity'].astype(int)\n",
    "    df['TotalLPProviders'] = df['TotalLPProviders'].astype(int)\n",
    "    df['RugScore'] = df['RugScore'].astype(int)\n",
    "\n",
    "    # Separate into different datasets\n",
    "    df_creation_lp = df[df['IsLP'] == True]\n",
    "    df_boost = df[df['IsLP'] == False]\n",
    "    \n",
    "    return df_creation_lp, df_boost\n",
    "\n",
    "\n",
    "df_lp, df_boost = initial_processing(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laú\\AppData\\Local\\Temp\\ipykernel_34128\\2715368307.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  price_data['PriceTime'] = pd.to_datetime(price_data['PriceTime'], errors='coerce').dt.tz_localize(None)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'StartPrice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'StartPrice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 85\u001b[0m\n\u001b[0;32m     79\u001b[0m df_to_parse \u001b[38;5;241m=\u001b[39m df_analysis\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     80\u001b[0m df_to_parse \u001b[38;5;241m=\u001b[39m df_to_parse[\n\u001b[0;32m     81\u001b[0m     (df_to_parse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStartLiquidityUSD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m10000\u001b[39m) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m     82\u001b[0m     (df_to_parse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRugScore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     83\u001b[0m ]\n\u001b[1;32m---> 85\u001b[0m price_evo_lp \u001b[38;5;241m=\u001b[39m \u001b[43mparse_price_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_to_parse\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 57\u001b[0m, in \u001b[0;36mparse_price_history\u001b[1;34m(df, tp, sl)\u001b[0m\n\u001b[0;32m     54\u001b[0m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeSinceBoostStart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimeSinceBoostStart\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInt64\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use Int64 which can handle NaN\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Calculate price variation percentage\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriceVariation_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ((df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[43mdf_expanded\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStartPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m/\u001b[39m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStartPrice\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     58\u001b[0m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriceVariation_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_expanded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriceVariation_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Assign triggers (Take Profit / Stop Loss)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Python\\my_projects\\dexboost\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'StartPrice'"
     ]
    }
   ],
   "source": [
    "def parse_price_history(df, tp=50, sl=-40):\n",
    "    \"\"\"\n",
    "    Expands the PriceHistory column into multiple rows while retaining original columns.\n",
    "    Calculates price variation percentage and time since the boost.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure BoostTime is in datetime format without timezone\n",
    "    df['DetectedAt'] = pd.to_datetime(df['DetectedAt'], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "    # Clean and format PriceHistory column before JSON parsing\n",
    "    df['PriceHistory'] = df['PriceHistory'].astype(str).str.replace(r'\\\\\"', '\"', regex=True).str.strip('\"')\n",
    "\n",
    "    # Remove NaN and empty values from PriceHistory\n",
    "    df = df[df['PriceHistory'].notna() & (df['PriceHistory'] != 'nan') & (df['PriceHistory'] != '')]\n",
    "\n",
    "    # Parse PriceHistory from string to JSON\n",
    "    df['PriceHistory'] = df['PriceHistory'].apply(json.loads)\n",
    "\n",
    "    # Remove rows where PriceHistory is an empty list\n",
    "    df = df[df['PriceHistory'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "    # Expand PriceHistory into separate rows while retaining original columns\n",
    "    df_expanded = df.explode('PriceHistory').reset_index(drop=True)\n",
    "\n",
    "    if df_expanded.empty:\n",
    "        return pd.DataFrame(columns=['TokenMint', 'TokenName', 'DetectedAt','price', 'PriceVariation_%', 'Trigger', 'TimeSinceBoostStart'])\n",
    "\n",
    "    # Normalize the JSON PriceHistory column into separate columns\n",
    "    price_data = pd.json_normalize(df_expanded['PriceHistory'])\n",
    "\n",
    "    if price_data.empty or 'price' not in price_data.columns or 'time' not in price_data.columns:\n",
    "        return pd.DataFrame(columns=['TokenMint', 'TokenName', 'PriceTime', 'price', 'Trigger', 'TimeSinceBoostStart'])\n",
    "\n",
    "    # Rename extracted columns\n",
    "    price_data.columns = ['price', 'PriceTime']\n",
    "\n",
    "    # Convert PriceTime and price to correct data types\n",
    "    price_data['PriceTime'] = pd.to_datetime(price_data['PriceTime'], errors='coerce').dt.tz_localize(None)\n",
    "    price_data['price'] = pd.to_numeric(price_data['price'], errors='coerce')\n",
    "\n",
    "    # Merge processed data with original DataFrame\n",
    "    df_expanded = df_expanded.drop(columns=['PriceHistory']).reset_index(drop=True)\n",
    "    df_expanded = pd.concat([df_expanded, price_data], axis=1)\n",
    "\n",
    "    # Ensure PriceHistory is correctly sorted before computing TimeSinceBoostStart\n",
    "    df_expanded = df_expanded.sort_values(by=['TokenMint', 'PriceTime'], ascending=True)\n",
    "\n",
    "    # Calculate time difference since boost (in minutes)\n",
    "    df_expanded['TimeSinceBoostStart'] = (df_expanded['PriceTime'] - df_expanded['DetectedAt']).dt.total_seconds()\n",
    "    df_expanded['TimeSinceBoostStart'] = df_expanded['TimeSinceBoostStart'].fillna(0)  # Fill NaN values with 0\n",
    "    df_expanded['TimeSinceBoostStart'] = df_expanded['TimeSinceBoostStart'].clip(lower=0)  # Clip negative values\n",
    "    df_expanded['TimeSinceBoostStart'] = df_expanded['TimeSinceBoostStart'].astype('Int64')  # Use Int64 which can handle NaN\n",
    "\n",
    "    # Calculate price variation percentage\n",
    "    df_expanded['PriceVariation_%'] = ((df_expanded['price'] - df_expanded['StartPrice']) / df_expanded['StartPrice']) * 100\n",
    "    df_expanded['PriceVariation_%'] = df_expanded['PriceVariation_%'].round(2)\n",
    "\n",
    "    # Assign triggers (Take Profit / Stop Loss)\n",
    "    df_expanded['Trigger'] = np.select(\n",
    "        [df_expanded['PriceVariation_%'] >= tp, df_expanded['PriceVariation_%'] <= sl],\n",
    "        ['TP', 'SL'], default='No event'\n",
    "    )\n",
    "\n",
    "    # Select relevant columns and define data types\n",
    "    cols = ['TokenMint', 'TokenName', 'price', 'PriceVariation_%', 'Trigger', 'TimeSinceBoostStart']\n",
    "\n",
    "    dtypes = {\n",
    "        \"TokenMint\": 'str',\n",
    "        \"TokenName\": 'str',\n",
    "        \"price\": 'float32',\n",
    "        \"TimeSinceBoostStart\": 'int64',\n",
    "        \"Trigger\": 'str'\n",
    "    }\n",
    "\n",
    "    return df_expanded[cols].astype(dtypes).reset_index(drop=True)\n",
    "\n",
    "df_to_parse = df_analysis.copy()\n",
    "df_to_parse = df_to_parse[\n",
    "    (df_to_parse['StartLiquidityUSD'] > 10000) &\n",
    "    (df_to_parse['RugScore'] <= 1000)\n",
    "]\n",
    "\n",
    "price_evo_lp = parse_price_history(df_to_parse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
